<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wangzichen的个人博客</title>
  
  <subtitle>记录点滴的日常</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.codedog.fun/"/>
  <updated>2019-08-10T00:02:56.183Z</updated>
  <id>http://www.codedog.fun/</id>
  
  <author>
    <name>WangZiChen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SparkSQL中RDD和DataFormat和DataSet的转换</title>
    <link href="http://www.codedog.fun/2019/08/10/RDD%E3%80%81DF%E3%80%81DS/"/>
    <id>http://www.codedog.fun/2019/08/10/RDD、DF、DS/</id>
    <published>2019-08-09T23:41:09.000Z</published>
    <updated>2019-08-10T00:02:56.183Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>RDD ——&gt;DF/DS</p><ul><li><p>——DF:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//第一种</span><br><span class="line">rdd.map&#123;</span><br><span class="line">  x=&gt;val pa = x.split(&quot;,&quot;);(pa(0).trim,pa(1).trim)</span><br><span class="line">&#125;.toDF(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line">//-------------------------------------------------------</span><br><span class="line">//第二种</span><br><span class="line">case class People(name:String,age:Integer)</span><br><span class="line">rdd.map&#123;</span><br><span class="line">  x=&gt;val pa=x.split(&quot;,&quot;);People(pa(0).trim,pa(1).trim)</span><br><span class="line">&#125;.toDF</span><br></pre></td></tr></table></figure></li><li><p>——DS:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">case class People(name:String,age:Integer)</span><br><span class="line">rdd.map&#123;</span><br><span class="line">  x=&gt;val pa=x.split(&quot;,&quot;);People(pa(0).trim,pa(1).trim)</span><br><span class="line">&#125;.toDS</span><br></pre></td></tr></table></figure></li></ul></li><li><p>DF——RDD/DF</p><ul><li>——RDD:    <font color="red">DF.rdd获取值，编译器不校验类型</font></li><li>——DS: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">case class People(name:String,age:Integer)</span><br><span class="line">DF.as[People]</span><br></pre></td></tr></table></figure></li></ul></li><li><p>DS——RDD/DF</p><ul><li>——RDD:  <font color="red">DS.rdd获取值，编译器校验类型</font></li><li>——DF:   <font color="red">DS.toDF即可</font></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;RDD ——&amp;gt;DF/DS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;——DF:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.codedog.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Git——The requested URL returned error 403</title>
    <link href="http://www.codedog.fun/2019/08/05/Git%E9%97%AE%E9%A2%98/"/>
    <id>http://www.codedog.fun/2019/08/05/Git问题/</id>
    <published>2019-08-05T07:27:44.000Z</published>
    <updated>2019-08-05T09:29:07.377Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h3><p>hexo部署的仓库需要换到另一个账号上，于是按照之前创建的步骤，修改了hexo安装目录下的<font color="red">deploy</font>的<font color="red">repo</font>地址，然而<font color="red">hexo  s</font>和<font color="red">hexo  g</font>没出错，但是<font color="red">hexo  d</font>时，会出错。<img src="/images/git01.png"></p><h3 id="二、问题分析"><a href="#二、问题分析" class="headerlink" title="二、问题分析"></a>二、问题分析</h3><ul><li>有可能是你真的没有权限</li><li>有可能是修改了git仓库密码和用户名，导致本地内存和硬盘中的用户名和密码不能用</li></ul><h3 id="三、问题处理"><a href="#三、问题处理" class="headerlink" title="三、问题处理"></a>三、问题处理</h3><ol><li>执行<code>git config --list</code> ，查看<font color="red">git</font>的配置信息<img src="/images/git02.png"></li></ol><p>图中的<code>user.email</code>和<code>user.name</code>分别是登录的邮箱和用户名。</p><ol start="2"><li>打开<strong>控制面板-</strong>&gt;<strong>用户账户</strong>-&gt;<strong>管理Windows凭据</strong>，向下看有<strong>普通凭据</strong>，找到关于git:<a href="https://github.com并删除。" target="_blank" rel="noopener">https://github.com并删除。</a></li><li>然而我是通过这种方法成功的。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//先去根目录</span><br><span class="line">root@wangxiaozhang:~/blog cd</span><br><span class="line">//再执行来查看credential中缓存的用户</span><br><span class="line">root@wangxiaozhang:~# vi .git-credentials</span><br></pre></td></tr></table></figure><p>这时我发现这里的代码是我准备要替换的账号名和密码，于是立刻改成现在用的账户名和密码，之后<strong>hexo  d</strong>成功！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、问题描述&quot;&gt;&lt;a href=&quot;#一、问题描述&quot; class=&quot;headerlink&quot; title=&quot;一、问题描述&quot;&gt;&lt;/a&gt;一、问题描述&lt;/h3&gt;&lt;p&gt;hexo部署的仓库需要换到另一个账号上，于是按照之前创建的步骤，修改了hexo安装目录下的&lt;font col
      
    
    </summary>
    
      <category term="Bug" scheme="http://www.codedog.fun/categories/Bug/"/>
    
    
      <category term="Git" scheme="http://www.codedog.fun/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Spark安装</title>
    <link href="http://www.codedog.fun/2019/08/05/Spark%E5%AE%89%E8%A3%85/"/>
    <id>http://www.codedog.fun/2019/08/05/Spark安装/</id>
    <published>2019-08-04T23:08:18.000Z</published>
    <updated>2019-08-05T00:02:40.790Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Spark安装地址"><a href="#Spark安装地址" class="headerlink" title="Spark安装地址"></a>Spark安装地址</h4><ul><li>官网地址</li></ul><p><a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><ul><li>文档查看地址</li></ul><p><a href="http://spark.apache.org/docs" target="_blank" rel="noopener">http://spark.apache.org/docs</a> </p><ul><li>下载地址</li></ul><p><a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">https://spark.apache.org/downloads.html</a></p><h4 id="Standalone模式安装"><a href="#Standalone模式安装" class="headerlink" title="Standalone模式安装"></a>Standalone模式安装</h4><ol><li>上传并解压spark安装包</li></ol><p><code>tar -zxvf /opt/softwares/spark-2.4.3-bin-hadoop2.7.tgz -C /opt/module/</code></p><ol start="2"><li>进入spark安装目录下的conf文件夹</li></ol><p><code>cd spark-2.4.3-bin-hadoop2.7/conf</code></p><ol start="3"><li>修改配置文件</li></ol><p><code>mv slaves.template slaves</code></p><p><code>mv spark-env.sh.template spark-env.sh</code></p><ol start="4"><li>修改slave文件，添加work节点</li></ol><p><code>vi slaves</code></p><p><code>hadoop101</code></p><p><code>hadoop102</code></p><p><code>hadoop103</code></p><ol start="5"><li>修改spark-env.sh文件，添加如下配置</li></ol><p><code>vi spark-env.sh</code></p><p><code>SPARK_MASTER_HOST=hadoop101</code></p><p><code>SPARK_MASTER_PROT=7077</code></p><ol start="6"><li>分发spark包到其他虚拟机</li></ol><p><code>xsync spark-2.4.3-bin-hadoop2.7</code></p><p><font color="red">(附分发shell脚本xsync)</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#2 获取文件名称</span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line">#3 获取上级目录到绝对路径</span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line">#4 获取当前用户名称</span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line">#5 循环</span><br><span class="line">for((host=102; host&lt;104; host++)); do</span><br><span class="line">        #echo $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">        echo --------------- hadoop$host ----------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ol start="7"><li>启动</li></ol><p><code>sbin/start-all.sh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[wangzichen@hadoop101 spark-2.4.3-bin-hadoop2.7]$ jps.sh </span><br><span class="line"></span><br><span class="line">================wangzichen@hadoop101================</span><br><span class="line"></span><br><span class="line">3330 Jps</span><br><span class="line"></span><br><span class="line">3238 Worker</span><br><span class="line"></span><br><span class="line">3163 Master</span><br><span class="line"></span><br><span class="line">================wangzichen@hadoop102================</span><br><span class="line"></span><br><span class="line">2966 Jps</span><br><span class="line"></span><br><span class="line">2908 Worker</span><br><span class="line"></span><br><span class="line">================wangzichen@hadoop103================</span><br><span class="line"></span><br><span class="line">2978 Worker</span><br><span class="line"></span><br><span class="line">3036 Jps</span><br></pre></td></tr></table></figure><ol start="8"><li>启动spark shell</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark-2.4.3-bin-hadoop2.7/bin/spark-shell \</span><br><span class="line">--master spark://hadoop101:7077 \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--total-executor-cores 2</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系 。</p><p><font color="red">Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可 。</font></p><h4 id="JobHistoryServer配置"><a href="#JobHistoryServer配置" class="headerlink" title="JobHistoryServer配置"></a>JobHistoryServer配置</h4><ol><li>修改spark-default.conf.template名称</li></ol><p><code>mv spark-defaults.conf.template spark-defaults.conf</code></p><ol start="2"><li>修改spark-default.conf文件，开启log</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           //true</span><br><span class="line">spark.eventLog.dir               //hdfs://hadoop101:9000/directory（若是hadoop中core-site.xml配置端口为8020则设置为8020而不是9000，否则出错）</span><br></pre></td></tr></table></figure><p><font color="red">注意：HDFS上的目录需要提前存在</font></p><ol start="3"><li>修改spark-env.sh文件，添加如下配置</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi spark-env.sh</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000 </span><br><span class="line">-Dspark.history.retainedApplications=3 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/directory&quot;      //若是hadoop中core-site.xml配置端口为8020则设置为8020而不是9000，否则出错</span><br></pre></td></tr></table></figure><ol start="4"><li>分发配置文件到其他虚拟机</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xsync spark-defaults.conf</span><br><span class="line">xsync spark-env.sh</span><br></pre></td></tr></table></figure><ol start="5"><li>启动历史服务</li></ol><p><code>sbin/start-history-server.sh</code></p><ol start="6"><li>测试，执行任务长度</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop101:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.4..jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><ol start="7"><li>查看历史服务</li></ol><p>在浏览器上输入<font color="red">hadoop101:4000</font>查看</p><h4 id="Yarn模式安装"><a href="#Yarn模式安装" class="headerlink" title="Yarn模式安装"></a>Yarn模式安装</h4><ol><li>修改hadoop配置文件yarn-site.xml，并添加如下内容</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vi yarn-site.xml</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>修改spark-env.sh，添加如下配置</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi spark-env.sh</span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop  </span><br><span class="line">HADOOP_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure><ol start="3"><li>分发配置文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml</span><br><span class="line">xsync spark-env.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Spark安装地址&quot;&gt;&lt;a href=&quot;#Spark安装地址&quot; class=&quot;headerlink&quot; title=&quot;Spark安装地址&quot;&gt;&lt;/a&gt;Spark安装地址&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;官网地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;http://s
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.codedog.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark初识</title>
    <link href="http://www.codedog.fun/2019/08/04/Spark%E5%88%9D%E8%AF%86/"/>
    <id>http://www.codedog.fun/2019/08/04/Spark初识/</id>
    <published>2019-08-04T14:42:17.000Z</published>
    <updated>2019-08-05T00:20:12.088Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark简介-官网的解释Spark官网"><a href="#Spark简介-官网的解释Spark官网" class="headerlink" title="Spark简介(官网的解释Spark官网)"></a>Spark简介(官网的解释<a href="http://spark.apache.org/" target="_blank" rel="noopener"><font color="blue">Spark官网</font></a>)</h2><img src="/images/what is spark.png"><p>Spark是一种<strong>快速</strong>、<strong>通用</strong>、<strong>可扩展</strong>的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。项目是用Scala进行编写。 </p><p>Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是基于内存计算的大数据并行计算框架。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark 使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分 别管理的负担。 </p><p>大一统的软件栈，各个组件关系密切并且可以相互调用，这种设计有几个好处：1、软件栈中所有的程序库和高级组件 都可以从下层的改进中获益。2、运行整个软件栈的代价变小了。不需要运 行 5 到 10 套独立的软件系统了，一个机构只需要运行一套软件系统即可。系统的部署、维护、测试、支持等大大缩减。3、能够构建出无缝整合不同处理模型的应用。</p><p>Spark内置项目如下：<img src="/images/spark01.png"></p><h4 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core:"></a>Spark Core:</h4><p>实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义 。</p><h4 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL:"></a>Spark SQL:</h4><p>是 Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。</p><h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming:"></a>Spark Streaming:</h4><p>是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。 </p><h4 id="Spark-MLlib"><a href="#Spark-MLlib" class="headerlink" title="Spark MLlib:"></a>Spark MLlib:</h4><p>提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。</p><h2 id="Spark特点"><a href="#Spark特点" class="headerlink" title="Spark特点"></a>Spark特点</h2><ol><li><font color="red"><strong>快</strong></font>：与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。 </li><li><font color="red"><strong>易用</strong></font>：Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。 </li><li><font color="red"><strong>通用</strong></font>：Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。 </li><li><font color="red"><strong>兼容性</strong></font>：Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。 </li></ol><h2 id="Spark角色及运行模式"><a href="#Spark角色及运行模式" class="headerlink" title="Spark角色及运行模式"></a>Spark角色及运行模式</h2><h4 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色<img src="/images/spark02.png"></h4><p>从物理部署层面上来看，Spark主要分为两种类型的节点，<font color="red"><strong>Master节点</strong></font>和<font color="red"><strong>Worker节点</strong></font>：Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。</p><p>从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。</p><h4 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h4><ol><li><p><strong>Local模式</strong>：Local模式就是运行在一台计算机上的模式，通常就是用于在本机上练手和测试。它可以通过以下集中方式设置master。 </p><p>local: 所有计算都运行在一个线程当中，没有任何并行计算，通常我们在本机执行一些测试代码，或者练手，就用这种模式;</p><p>local[K]: 指定使用几个线程来运行计算，比如local[4]就是运行4个worker线程。通常我们的cpu有几个core，就指定几个线程，最大化利用cpu的计算能力;</p><p>local[*]: 这种模式直接帮你按照cpu最多cores来设置线程数了。</p></li><li><p><strong>Standalone模式</strong>：构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。 </p></li><li><p><strong>Yarn模式</strong>：Spark客户端直接连接Yarn；不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。</p><p>yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。</p><p>yarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AP（APPMaster）适用于生产环境。</p></li><li><p><strong>Mesos模式</strong>：Spark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。 </p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Spark简介-官网的解释Spark官网&quot;&gt;&lt;a href=&quot;#Spark简介-官网的解释Spark官网&quot; class=&quot;headerlink&quot; title=&quot;Spark简介(官网的解释Spark官网)&quot;&gt;&lt;/a&gt;Spark简介(官网的解释&lt;a href=&quot;htt
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.codedog.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
</feed>
