<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wangzichen的个人博客</title>
  
  <subtitle>记录点滴的日常</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.codedog.fun/"/>
  <updated>2019-09-01T06:59:19.210Z</updated>
  <id>http://www.codedog.fun/</id>
  
  <author>
    <name>WangZiChen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://www.codedog.fun/2019/09/01/Docker%E8%BF%9E%E6%8E%A5/"/>
    <id>http://www.codedog.fun/2019/09/01/Docker连接/</id>
    <published>2019-09-01T06:59:19.210Z</published>
    <updated>2019-09-01T06:59:19.210Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>IDEA导入项目报错non-managed pom.xml file found</title>
    <link href="http://www.codedog.fun/2019/08/16/IDEA-bug/"/>
    <id>http://www.codedog.fun/2019/08/16/IDEA-bug/</id>
    <published>2019-08-16T00:04:26.000Z</published>
    <updated>2019-08-16T00:10:38.589Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一步：点击右侧的maven视图"><a href="#第一步：点击右侧的maven视图" class="headerlink" title="第一步：点击右侧的maven视图"></a>第一步：点击右侧的maven视图</h2><image src="/images/20180822160605830.png"><h2 id="第二步：选择需要添加的pox-xml文件"><a href="#第二步：选择需要添加的pox-xml文件" class="headerlink" title="第二步：选择需要添加的pox.xml文件"></a>第二步：选择需要添加的pox.xml文件</h2><image src="/images/20180822161016696.png"><h2 id="第三步：选择项目运行需要的SDK"><a href="#第三步：选择项目运行需要的SDK" class="headerlink" title="第三步：选择项目运行需要的SDK"></a>第三步：选择项目运行需要的SDK</h2><image src="/images/20180822161413925.png"><h4 id="OVER"><a href="#OVER" class="headerlink" title="OVER"></a><font colot="red">OVER</font></h4></image></image></image>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;第一步：点击右侧的maven视图&quot;&gt;&lt;a href=&quot;#第一步：点击右侧的maven视图&quot; class=&quot;headerlink&quot; title=&quot;第一步：点击右侧的maven视图&quot;&gt;&lt;/a&gt;第一步：点击右侧的maven视图&lt;/h2&gt;&lt;image src=&quot;/imag
      
    
    </summary>
    
      <category term="Bug" scheme="http://www.codedog.fun/categories/Bug/"/>
    
    
      <category term="IDEA" scheme="http://www.codedog.fun/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>RDD认知(下)</title>
    <link href="http://www.codedog.fun/2019/08/12/RDD%E7%9A%84%E8%AE%A4%E7%9F%A5(%E4%B8%8B)/"/>
    <id>http://www.codedog.fun/2019/08/12/RDD的认知(下)/</id>
    <published>2019-08-11T23:16:57.000Z</published>
    <updated>2019-08-12T02:43:46.337Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、转换操作-续"><a href="#一、转换操作-续" class="headerlink" title="一、转换操作(续)"></a>一、转换操作(续)</h2><table><thead><tr><th align="left">函数</th><th align="left">解释</th><th align="left">示例</th></tr></thead><tbody><tr><td align="left"><code>def aggregateByKey[U:ClassTag](zeroValue:U,partitioner:Partitioner)(seqOp:(U,V)=&gt;U,combOp:(U,U)=&gt;U):RDD[(K,U)]</code></td><td align="left">通过seqOp函数将每一个分区里面的数据和初始值迭代带入函数返回最终值，comOp将每一个分区返回的最终值根据key进行合并操作</td><td align="left">略</td></tr><tr><td align="left"><code>def foldByKey(zeroValue:V,partitioner:Partitioner)(func:(V,V)=&gt;V):RDD[(K,V)]</code></td><td align="left">aggregateByKey的简化操作，seqop和combop相同</td><td align="left">val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)<br>val agg = rdd.foldByKey(0)(<em>+</em>).collect</td></tr><tr><td align="left"><code>def sortByKey(ascending:Boolean=true,numPartitions:Int=self.partitions.length):RDD[(K,V)]</code></td><td align="left">在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</td><td align="left">val rdd = sc.parallelize(Array((3,”aa”),(6,”cc”),(2,”bb”),(1,”dd”))) <br> rdd.sortByKey(true).collect()</td></tr><tr><td align="left"><code>def sortBy[K](f:(T)=&gt;K,ascending:Boolean=true,numPartitions:Int = this.partitions.length)(implicit ord:Ordering[K],ctag:ClassTag[K]):RDD[T]</code></td><td align="left">底层实现还是使用sortByKey，只不过使用fun生成的新key进行排序</td><td align="left">val rdd = sc.parallelize(List(1,2,3,4))<br> rdd.sortBy(x =&gt; x%3).collect()</td></tr><tr><td align="left"><code>def join[W](other:RDD[(K,W)],partitioner:Partitioner):RDD[(K,(V,W))]</code></td><td align="left">在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</td><td align="left">val rdd1= sc.parallelize(Array((1,”a”),(2,”b”),(3,”c”))))<br> val rdd2 = sc.parallelize(Array((1,4),(2,5),(3,6)))<br> rdd1.join(rdd2).collect()</td></tr><tr><td align="left"><code>def cogroup[W](other:RDD[(K,W)],partitioner:Partitioner):RDD[(K,(Iterable[V], Iterable[W]))]</code></td><td align="left">在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></td><td align="left">略</td></tr><tr><td align="left"><code>def cartesian[U:ClassTag](other:RDD[U]):RDD[(T,U)]</code></td><td align="left">笛卡尔积</td><td align="left">val rdd1 = sc.parallelize(1 to 3)<br> val rdd2 = sc.parallelize(2 to 5)<br> rdd1.cartesian(rdd2).collect()</td></tr><tr><td align="left"><code>def pipe(command:String):RDD[String]</code></td><td align="left">对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD</td><td align="left">略</td></tr><tr><td align="left"><code>def coalesce(numPartitions:Int,shuffle:Boolean=false,partitionCoalescer: Option[PartitionCoalescer]=Option.empty)(implicit ord:Ordering[T]=null):RDD[T]</code></td><td align="left">缩减分区数，用于大数据集过滤后，提高小数据集的执行效率</td><td align="left">略</td></tr><tr><td align="left"><code>def repartition(numPartitions:Int)(implicit ord:Ordering[T]=null):RDD[T]</code></td><td align="left">根据分区数,重新通过网络随机洗牌所有数据</td><td align="left">略</td></tr><tr><td align="left"><code>def repartitionAndSortWithinPartitions(partitioner:Partitioner):RDD[(K,V)]</code></td><td align="left">repartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高</td><td align="left">略</td></tr><tr><td align="left"><code>def glom():RDD[Array[T]]</code></td><td align="left">将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]</td><td align="left">val rdd = sc.parallelize(1 to 16,4)<br> rdd.glom().collect()</td></tr><tr><td align="left"><code>def mapValues[U](f:V=&gt;U):RDD[(K,U)]</code></td><td align="left">针对于(K,V)形式的类型只对V进行操作</td><td align="left">val rdd3 = sc.parallelize(Array((1,”a”),(1,”d”),(2,”b”),(3,”c”)))<br> rdd3.mapValues(_+”|||”).collect()</td></tr><tr><td align="left"><code>def subtract(other:RDD[T]):RDD[T]</code></td><td align="left">计算差的一种函数去除两个RDD中相同的元素，不同的RDD将保留下来</td><td align="left">val rdd = sc.parallelize(3 to 8) <br>val rdd1 = sc.parallelize(1 to 5) <br>rdd.subtract(rdd1).collect()</td></tr></tbody></table><h2 id="二、行动操作"><a href="#二、行动操作" class="headerlink" title="二、行动操作"></a>二、行动操作</h2><table><thead><tr><th>函数</th><th>解释</th><th>示例</th></tr></thead><tbody><tr><td><code>def takeSample(withReplacement:Boolean,num:Int,seed:Long=Utils.random.nextLong):Array[T]</code></td><td>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</td><td>val rdd=sc.parallelize(1 to 10,2)<br>rdd.takeSample(true,5,3)</td></tr><tr><td><code>def reduce(f:(T,T)=&gt;T):T</code></td><td>通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的</td><td>val rdd1 = sc.makeRDD(1 to 10,2)<br> rdd1.reduce(<em>+</em>)</td></tr><tr><td><code>def collect():Array[T]</code></td><td>在驱动程序中，以数组的形式返回数据集的所有元素</td><td>太常用，略</td></tr><tr><td><code>def count():Long</code></td><td>返回RDD的元素个数</td><td>略</td></tr><tr><td><code>def first():T</code></td><td>返回RDD的第一个元素（类似于take(1))</td><td>略</td></tr><tr><td><code>def take(num:Int):Array[T]</code></td><td>返回一个由数据集的前n个元素组成的数组</td><td>略</td></tr><tr><td><code>def takeOrdered(num:Int)(implicit ord:Ordering[T])</code></td><td>返回前几个的排序</td><td>略</td></tr><tr><td><code>def aggregate[U:ClassTag](zeroValue:U)(seqOp:(U,T)=&gt;U,combOp:(U,U)=&gt;U):U</code></td><td>aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致</td><td>scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)<br> scala&gt;rdd1.aggregate(1){(x : Int,y : Int) =&gt; x + y}, {(a : Int,b : Int) =&gt; a + b})</td></tr><tr><td><code>def fold(zeroValue:T)(op:(T,T)=&gt;T):T</code></td><td>折叠操作，aggregate的简化操作，seqop和combop一样</td><td>var rdd1 = sc.makeRDD(1 to 4,2)<br> rdd1.fold(1)(<em>+</em>)</td></tr><tr><td><code>def saveAsTextFile(path:String):Unit</code></td><td>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</td><td>略</td></tr><tr><td><code>def saveAsObjectFile(path:String):Unit</code></td><td>用于将RDD中的元素序列化成对象，存储到文件中</td><td>略</td></tr><tr><td><code>def countByKey():Map[K,Long]</code></td><td>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数</td><td>val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)<br>rdd.countByKey()</td></tr><tr><td><code>def foreach(f:T=&gt;Unit):Unit</code></td><td>在数据集的每一个元素上，运行函数func进行更新</td><td>var rdd = sc.makeRDD(1 to 10,2)<br> var sum = sc.accumulator(0)<br> rdd.foreach(sum+=_)</td></tr></tbody></table><h3 id="注意：当在RDD中使用到了class的方法或者属性的时候，该class需要继承java-io-Serializable接口，或者可以将属性赋值为本地变量来防止整个对象的传输"><a href="#注意：当在RDD中使用到了class的方法或者属性的时候，该class需要继承java-io-Serializable接口，或者可以将属性赋值为本地变量来防止整个对象的传输" class="headerlink" title="注意：当在RDD中使用到了class的方法或者属性的时候，该class需要继承java.io.Serializable接口，或者可以将属性赋值为本地变量来防止整个对象的传输 "></a><font color="red"><strong>注意：当在RDD中使用到了class的方法或者属性的时候，该class需要继承java.io.</strong>Serializable<strong>接口，或者可以将属性赋值为本地变量来防止整个对象的传输</strong> </font></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、转换操作-续&quot;&gt;&lt;a href=&quot;#一、转换操作-续&quot; class=&quot;headerlink&quot; title=&quot;一、转换操作(续)&quot;&gt;&lt;/a&gt;一、转换操作(续)&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;left&quot;&gt;函数&lt;/th&gt;
&lt;
      
    
    </summary>
    
      <category term="知识点" scheme="http://www.codedog.fun/categories/%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>RDD的认知（上）</title>
    <link href="http://www.codedog.fun/2019/08/11/RDD%E7%9A%84%E8%AE%A4%E7%9F%A5(%E4%B8%8A)/"/>
    <id>http://www.codedog.fun/2019/08/11/RDD的认知(上)/</id>
    <published>2019-08-11T02:45:19.000Z</published>
    <updated>2019-08-11T23:52:04.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、转换操作"><a href="#一、转换操作" class="headerlink" title="一、转换操作"></a>一、转换操作</h2><table><thead><tr><th align="left">函数</th><th align="left">解释</th><th align="left">案例</th></tr></thead><tbody><tr><td align="left"><code>def map[U:ClassTag](f:T =&gt; U):RDD[U]</code></td><td align="left">将函数应用于RDD的每一个元素，并返回一个新的RDD</td><td align="left">val rdd=sc.makeRDD(Array(1,2,3,4,5))   <br>  rdd.map((_,1)).collect</td></tr><tr><td align="left"><code>def filter(f:T =&gt; Boolean):RDD[T]</code></td><td align="left">通过提供的产生boolean条件的表达式来返回符合结果为True的新的RDD</td><td align="left">val rdd=sc.makeRDD(Array(“aa1”,”aa2”,”bb1”,”bb2”)) <br>   rdd.filter(_.startsWith(“aa”)).collect</td></tr><tr><td align="left"><code>def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]</code></td><td align="left">将函数应用于RDD中的每一项，对于每一项都产生一个集合，并将集合中的元素压扁成一个集合</td><td align="left">val rdd=makeRDD(1 to 3) <br>       rdd.flatMap((1 to _)).collect</td></tr><tr><td align="left"><code>def mapPartitions[U:ClassTag](f:Iterator[T] =&gt;Iterator[U],preservesPartitioning:Boolean=false): RDD[U]</code></td><td align="left">将函数应用于RDD的每一个分区，每一个分区运行一次，函数需要能够接受Iterator类型，然后返回Iterator</td><td align="left">略</td></tr><tr><td align="left"><code>def mapPartitionsWithIndex[U:ClassTag](f:(Int,Iterator[T])=&gt;Iterator[U],preservesPartitioning:Boolean =false):RDD[U]</code></td><td align="left">将函数应用于RDD中的每一个分区，每一个分区运行一次，函数能够接受 一个分区的索引值 和一个代表分区内所有数据的Iterator类型，需要返回Iterator类型</td><td align="left">略</td></tr><tr><td align="left"><code>def sample(withReplacement: Boolean,fraction:Double,seed:Long=Utils.random.nextLong): RDD[T]</code></td><td align="left">在RDD中以seed为种子返回大致上有fraction比例个数据样本RDD，withReplacement表示是否采用放回式抽样</td><td align="left">val sample=sc.makeRDD(1 to 100)    <br> sample.sample(false,0.1,4).collect   <br> sample.sample(true,0.1,4).collect</td></tr><tr><td align="left"><code>def union(other:RDD[T]):RDD[T]</code></td><td align="left">将两个RDD中的元素进行合并，返回一个新的RDD</td><td align="left">val a=sc.makeRDD(1 to 10)   <br>                       sc.makeRDD(5 to 15).union(a).collect</td></tr><tr><td align="left"><code>def intersection(other:RDD[T]):RDD[T]</code></td><td align="left">将两个RDD做交集，返回一个新的RDD</td><td align="left">val a=sc.makeRDD(1 to 10)   <br> val b=sc.makeRDD(5 to 15)    <br> a.intersection(b).collect</td></tr><tr><td align="left"><code>def distinct():RDD[T]</code></td><td align="left">将当前RDD进行去重后，返回一个新的RDD</td><td align="left">val dis=sc.makeRDD(List(1,2,3,4,5,2,3,4,1,6,4)） <br>  dis.distinct.collect</td></tr><tr><td align="left"><code>def partitionBy(partitioner:Partitioner):RDD[(K,V)]</code></td><td align="left">根据设置的分区器重新将RDD进行分区，返回新的RDD</td><td align="left">val par=sc.makeRDD(List((1,”a”),(2,”b”),(3,”c”)),4） <br>  par.partitionBy(new org.apache.spark.HashPatitioner(2))</td></tr><tr><td align="left"><code>def reduceByKey(func:(V,V)=&gt;V):RDD[(K,V)]</code></td><td align="left">根据Key值将相同Key的元组的值用func进行计算，返回新的RDD</td><td align="left">val rdd = sc.parallelize(Array(1,2,3,1,2))   <br> rdd.map((<em>,1))   res0.reduceByKey(</em>+_)</td></tr><tr><td align="left"><code>def groupByKey():RDD[(K,Iterable[V])]</code></td><td align="left">将相同Key的值进行聚集，输出一个(K, Iterable[V])类型的RDD</td><td align="left">val words=Array(“one”,”one”,”two”,”three”,”three”,”three”) <br>  val wordPair=sc.parallelize(words).map((_,1)) <br>   val group=wordPair.groupByKey()   <br>  val map=group.map(t=&gt;(t._1,t._2.sum))</td></tr><tr><td align="left"><code>def combineByKey[C](createCombiner:V=&gt;C,mergeValue:(C,V)=&gt; C,mergeCombiners:(C,C)=&gt;C,numPartitions:Int):RDD[(K,C)]</code></td><td align="left">对相同K，把V合并成一个集合.<br>createCombiner: combineByKey()会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素,combineByKey()会使用一个叫作 createCombiner() 的函数来创建那个键对应的累加器的初始值<br>mergeValue:如果这是一个在处理当前分区之前已经遇到的键,它会使用 mergeValue()方法将该键的累加器对应的当前值与这个新的值进行合并<br>mergeCombiners:由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的 mergeCombiners()方法将各个分区的结果进行合并。</td><td align="left"><code>scala&gt; val scores = Array((&quot;Fred&quot;, 88), (&quot;Fred&quot;, 95), (&quot;Fred&quot;, 91), (&quot;Wilma&quot;, 93), (&quot;Wilma&quot;, 95), (&quot;Wilma&quot;, 98))</code> <code>scores: Array[(String, Int)] = Array((Fred,88), (Fred,95), (Fred,91), (Wilma,93), (Wilma,95), (Wilma,98))</code><br> <code></code> <code>scala&gt; val input = sc.parallelize(scores)</code> <code>input: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:26</code><br> <code></code> <code>scala&gt; val combine = input.combineByKey(</code> `</td></tr></tbody></table><h3 id="待续"><a href="#待续" class="headerlink" title="待续"></a><font color="blue">待续</font></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、转换操作&quot;&gt;&lt;a href=&quot;#一、转换操作&quot; class=&quot;headerlink&quot; title=&quot;一、转换操作&quot;&gt;&lt;/a&gt;一、转换操作&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;left&quot;&gt;函数&lt;/th&gt;
&lt;th align=&quot;
      
    
    </summary>
    
      <category term="知识点" scheme="http://www.codedog.fun/categories/%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkSQL中RDD和DataFrame和DataSet的转换</title>
    <link href="http://www.codedog.fun/2019/08/10/RDD%E3%80%81DF%E3%80%81DS/"/>
    <id>http://www.codedog.fun/2019/08/10/RDD、DF、DS/</id>
    <published>2019-08-09T23:41:09.000Z</published>
    <updated>2019-08-11T23:16:36.223Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>RDD ——&gt;DF/DS</p><ul><li><p>——DF:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//第一种</span><br><span class="line">rdd.map&#123;</span><br><span class="line">  x=&gt;val pa = x.split(&quot;,&quot;);(pa(0).trim,pa(1).trim)</span><br><span class="line">&#125;.toDF(&quot;name&quot;,&quot;age&quot;)</span><br><span class="line">//-------------------------------------------------------</span><br><span class="line">//第二种</span><br><span class="line">case class People(name:String,age:Integer)</span><br><span class="line">rdd.map&#123;</span><br><span class="line">  x=&gt;val pa=x.split(&quot;,&quot;);People(pa(0).trim,pa(1).trim)</span><br><span class="line">&#125;.toDF</span><br></pre></td></tr></table></figure></li><li><p>——DS:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">case class People(name:String,age:Integer)</span><br><span class="line">rdd.map&#123;</span><br><span class="line">  x=&gt;val pa=x.split(&quot;,&quot;);People(pa(0).trim,pa(1).trim)</span><br><span class="line">&#125;.toDS</span><br></pre></td></tr></table></figure></li></ul></li><li><p>DF——RDD/DF</p><ul><li>——RDD:    <font color="red">DF.rdd获取值，编译器不校验类型</font></li><li>——DS: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">case class People(name:String,age:Integer)</span><br><span class="line">DF.as[People]</span><br></pre></td></tr></table></figure></li></ul></li><li><p>DS——RDD/DF</p><ul><li>——RDD:  <font color="red">DS.rdd获取值，编译器校验类型</font></li><li>——DF:   <font color="red">DS.toDF即可</font></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;RDD ——&amp;gt;DF/DS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;——DF:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;
      
    
    </summary>
    
      <category term="知识点" scheme="http://www.codedog.fun/categories/%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Git——The requested URL returned error 403</title>
    <link href="http://www.codedog.fun/2019/08/05/Git%E9%97%AE%E9%A2%98/"/>
    <id>http://www.codedog.fun/2019/08/05/Git问题/</id>
    <published>2019-08-05T07:27:44.000Z</published>
    <updated>2019-08-05T09:29:07.377Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h3><p>hexo部署的仓库需要换到另一个账号上，于是按照之前创建的步骤，修改了hexo安装目录下的<font color="red">deploy</font>的<font color="red">repo</font>地址，然而<font color="red">hexo  s</font>和<font color="red">hexo  g</font>没出错，但是<font color="red">hexo  d</font>时，会出错。<img src="/images/git01.png"></p><h3 id="二、问题分析"><a href="#二、问题分析" class="headerlink" title="二、问题分析"></a>二、问题分析</h3><ul><li>有可能是你真的没有权限</li><li>有可能是修改了git仓库密码和用户名，导致本地内存和硬盘中的用户名和密码不能用</li></ul><h3 id="三、问题处理"><a href="#三、问题处理" class="headerlink" title="三、问题处理"></a>三、问题处理</h3><ol><li>执行<code>git config --list</code> ，查看<font color="red">git</font>的配置信息<img src="/images/git02.png"></li></ol><p>图中的<code>user.email</code>和<code>user.name</code>分别是登录的邮箱和用户名。</p><ol start="2"><li>打开<strong>控制面板-</strong>&gt;<strong>用户账户</strong>-&gt;<strong>管理Windows凭据</strong>，向下看有<strong>普通凭据</strong>，找到关于git:<a href="https://github.com并删除。" target="_blank" rel="noopener">https://github.com并删除。</a></li><li>然而我是通过这种方法成功的。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//先去根目录</span><br><span class="line">root@wangxiaozhang:~/blog cd</span><br><span class="line">//再执行来查看credential中缓存的用户</span><br><span class="line">root@wangxiaozhang:~# vi .git-credentials</span><br></pre></td></tr></table></figure><p>这时我发现这里的代码是我准备要替换的账号名和密码，于是立刻改成现在用的账户名和密码，之后<strong>hexo  d</strong>成功！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、问题描述&quot;&gt;&lt;a href=&quot;#一、问题描述&quot; class=&quot;headerlink&quot; title=&quot;一、问题描述&quot;&gt;&lt;/a&gt;一、问题描述&lt;/h3&gt;&lt;p&gt;hexo部署的仓库需要换到另一个账号上，于是按照之前创建的步骤，修改了hexo安装目录下的&lt;font col
      
    
    </summary>
    
      <category term="Bug" scheme="http://www.codedog.fun/categories/Bug/"/>
    
    
      <category term="Git" scheme="http://www.codedog.fun/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Spark安装</title>
    <link href="http://www.codedog.fun/2019/08/05/Spark%E5%AE%89%E8%A3%85/"/>
    <id>http://www.codedog.fun/2019/08/05/Spark安装/</id>
    <published>2019-08-04T23:08:18.000Z</published>
    <updated>2019-08-05T00:02:40.790Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Spark安装地址"><a href="#Spark安装地址" class="headerlink" title="Spark安装地址"></a>Spark安装地址</h4><ul><li>官网地址</li></ul><p><a href="http://spark.apache.org" target="_blank" rel="noopener">http://spark.apache.org</a></p><ul><li>文档查看地址</li></ul><p><a href="http://spark.apache.org/docs" target="_blank" rel="noopener">http://spark.apache.org/docs</a> </p><ul><li>下载地址</li></ul><p><a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">https://spark.apache.org/downloads.html</a></p><h4 id="Standalone模式安装"><a href="#Standalone模式安装" class="headerlink" title="Standalone模式安装"></a>Standalone模式安装</h4><ol><li>上传并解压spark安装包</li></ol><p><code>tar -zxvf /opt/softwares/spark-2.4.3-bin-hadoop2.7.tgz -C /opt/module/</code></p><ol start="2"><li>进入spark安装目录下的conf文件夹</li></ol><p><code>cd spark-2.4.3-bin-hadoop2.7/conf</code></p><ol start="3"><li>修改配置文件</li></ol><p><code>mv slaves.template slaves</code></p><p><code>mv spark-env.sh.template spark-env.sh</code></p><ol start="4"><li>修改slave文件，添加work节点</li></ol><p><code>vi slaves</code></p><p><code>hadoop101</code></p><p><code>hadoop102</code></p><p><code>hadoop103</code></p><ol start="5"><li>修改spark-env.sh文件，添加如下配置</li></ol><p><code>vi spark-env.sh</code></p><p><code>SPARK_MASTER_HOST=hadoop101</code></p><p><code>SPARK_MASTER_PROT=7077</code></p><ol start="6"><li>分发spark包到其他虚拟机</li></ol><p><code>xsync spark-2.4.3-bin-hadoop2.7</code></p><p><font color="red">(附分发shell脚本xsync)</font></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#1 获取输入参数个数，如果没有参数，直接退出</span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#2 获取文件名称</span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line">#3 获取上级目录到绝对路径</span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line">#4 获取当前用户名称</span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line">#5 循环</span><br><span class="line">for((host=102; host&lt;104; host++)); do</span><br><span class="line">        #echo $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">        echo --------------- hadoop$host ----------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ol start="7"><li>启动</li></ol><p><code>sbin/start-all.sh</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[wangzichen@hadoop101 spark-2.4.3-bin-hadoop2.7]$ jps.sh </span><br><span class="line"></span><br><span class="line">================wangzichen@hadoop101================</span><br><span class="line"></span><br><span class="line">3330 Jps</span><br><span class="line"></span><br><span class="line">3238 Worker</span><br><span class="line"></span><br><span class="line">3163 Master</span><br><span class="line"></span><br><span class="line">================wangzichen@hadoop102================</span><br><span class="line"></span><br><span class="line">2966 Jps</span><br><span class="line"></span><br><span class="line">2908 Worker</span><br><span class="line"></span><br><span class="line">================wangzichen@hadoop103================</span><br><span class="line"></span><br><span class="line">2978 Worker</span><br><span class="line"></span><br><span class="line">3036 Jps</span><br></pre></td></tr></table></figure><ol start="8"><li>启动spark shell</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark-2.4.3-bin-hadoop2.7/bin/spark-shell \</span><br><span class="line">--master spark://hadoop101:7077 \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--total-executor-cores 2</span><br></pre></td></tr></table></figure><p><font color="red">注意：</font>如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系 。</p><p><font color="red">Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可 。</font></p><h4 id="JobHistoryServer配置"><a href="#JobHistoryServer配置" class="headerlink" title="JobHistoryServer配置"></a>JobHistoryServer配置</h4><ol><li>修改spark-default.conf.template名称</li></ol><p><code>mv spark-defaults.conf.template spark-defaults.conf</code></p><ol start="2"><li>修改spark-default.conf文件，开启log</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled           //true</span><br><span class="line">spark.eventLog.dir               //hdfs://hadoop101:9000/directory（若是hadoop中core-site.xml配置端口为8020则设置为8020而不是9000，否则出错）</span><br></pre></td></tr></table></figure><p><font color="red">注意：HDFS上的目录需要提前存在</font></p><ol start="3"><li>修改spark-env.sh文件，添加如下配置</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi spark-env.sh</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=4000 </span><br><span class="line">-Dspark.history.retainedApplications=3 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://hadoop101:9000/directory&quot;      //若是hadoop中core-site.xml配置端口为8020则设置为8020而不是9000，否则出错</span><br></pre></td></tr></table></figure><ol start="4"><li>分发配置文件到其他虚拟机</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xsync spark-defaults.conf</span><br><span class="line">xsync spark-env.sh</span><br></pre></td></tr></table></figure><ol start="5"><li>启动历史服务</li></ol><p><code>sbin/start-history-server.sh</code></p><ol start="6"><li>测试，执行任务长度</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://hadoop101:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.4..jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><ol start="7"><li>查看历史服务</li></ol><p>在浏览器上输入<font color="red">hadoop101:4000</font>查看</p><h4 id="Yarn模式安装"><a href="#Yarn模式安装" class="headerlink" title="Yarn模式安装"></a>Yarn模式安装</h4><ol><li>修改hadoop配置文件yarn-site.xml，并添加如下内容</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vi yarn-site.xml</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>修改spark-env.sh，添加如下配置</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vi spark-env.sh</span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop  </span><br><span class="line">HADOOP_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop</span><br></pre></td></tr></table></figure><ol start="3"><li>分发配置文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/hadoop-2.7.2/etc/hadoop/yarn-site.xml</span><br><span class="line">xsync spark-env.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Spark安装地址&quot;&gt;&lt;a href=&quot;#Spark安装地址&quot; class=&quot;headerlink&quot; title=&quot;Spark安装地址&quot;&gt;&lt;/a&gt;Spark安装地址&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;官网地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;http://s
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.codedog.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark初识</title>
    <link href="http://www.codedog.fun/2019/08/04/Spark%E5%88%9D%E8%AF%86/"/>
    <id>http://www.codedog.fun/2019/08/04/Spark初识/</id>
    <published>2019-08-04T14:42:17.000Z</published>
    <updated>2019-08-05T00:20:12.088Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark简介-官网的解释Spark官网"><a href="#Spark简介-官网的解释Spark官网" class="headerlink" title="Spark简介(官网的解释Spark官网)"></a>Spark简介(官网的解释<a href="http://spark.apache.org/" target="_blank" rel="noopener"><font color="blue">Spark官网</font></a>)</h2><img src="/images/what is spark.png"><p>Spark是一种<strong>快速</strong>、<strong>通用</strong>、<strong>可扩展</strong>的大数据分析引擎，2009年诞生于加州大学伯克利分校AMPLab，2010年开源，2013年6月成为Apache孵化项目，2014年2月成为Apache顶级项目。项目是用Scala进行编写。 </p><p>Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是基于内存计算的大数据并行计算框架。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark 使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分 别管理的负担。 </p><p>大一统的软件栈，各个组件关系密切并且可以相互调用，这种设计有几个好处：1、软件栈中所有的程序库和高级组件 都可以从下层的改进中获益。2、运行整个软件栈的代价变小了。不需要运 行 5 到 10 套独立的软件系统了，一个机构只需要运行一套软件系统即可。系统的部署、维护、测试、支持等大大缩减。3、能够构建出无缝整合不同处理模型的应用。</p><p>Spark内置项目如下：<img src="/images/spark01.png"></p><h4 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core:"></a>Spark Core:</h4><p>实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义 。</p><h4 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL:"></a>Spark SQL:</h4><p>是 Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比 如 Hive 表、Parquet 以及 JSON 等。</p><h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming:"></a>Spark Streaming:</h4><p>是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。 </p><h4 id="Spark-MLlib"><a href="#Spark-MLlib" class="headerlink" title="Spark MLlib:"></a>Spark MLlib:</h4><p>提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。</p><h2 id="Spark特点"><a href="#Spark特点" class="headerlink" title="Spark特点"></a>Spark特点</h2><ol><li><font color="red"><strong>快</strong></font>：与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。 </li><li><font color="red"><strong>易用</strong></font>：Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。 </li><li><font color="red"><strong>通用</strong></font>：Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。 </li><li><font color="red"><strong>兼容性</strong></font>：Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。 </li></ol><h2 id="Spark角色及运行模式"><a href="#Spark角色及运行模式" class="headerlink" title="Spark角色及运行模式"></a>Spark角色及运行模式</h2><h4 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色<img src="/images/spark02.png"></h4><p>从物理部署层面上来看，Spark主要分为两种类型的节点，<font color="red"><strong>Master节点</strong></font>和<font color="red"><strong>Worker节点</strong></font>：Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。</p><p>从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。</p><h4 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h4><ol><li><p><strong>Local模式</strong>：Local模式就是运行在一台计算机上的模式，通常就是用于在本机上练手和测试。它可以通过以下集中方式设置master。 </p><p>local: 所有计算都运行在一个线程当中，没有任何并行计算，通常我们在本机执行一些测试代码，或者练手，就用这种模式;</p><p>local[K]: 指定使用几个线程来运行计算，比如local[4]就是运行4个worker线程。通常我们的cpu有几个core，就指定几个线程，最大化利用cpu的计算能力;</p><p>local[*]: 这种模式直接帮你按照cpu最多cores来设置线程数了。</p></li><li><p><strong>Standalone模式</strong>：构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。 </p></li><li><p><strong>Yarn模式</strong>：Spark客户端直接连接Yarn；不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。</p><p>yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。</p><p>yarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AP（APPMaster）适用于生产环境。</p></li><li><p><strong>Mesos模式</strong>：Spark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。 </p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Spark简介-官网的解释Spark官网&quot;&gt;&lt;a href=&quot;#Spark简介-官网的解释Spark官网&quot; class=&quot;headerlink&quot; title=&quot;Spark简介(官网的解释Spark官网)&quot;&gt;&lt;/a&gt;Spark简介(官网的解释&lt;a href=&quot;htt
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.codedog.fun/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://www.codedog.fun/tags/Spark/"/>
    
  </entry>
  
</feed>
